# Visualization-Regression-and-Classification

## Data Visualization and K-Nearest Neighbors (KNN) Cross-Validation Project
### Objective: 
Performed exploratory data analysis and visualization using Seaborn and Matplotlib to uncover patterns in two datasets.
### Techniques Used: 
Implemented various plots (KDE, box, violin, histogram) for feature visualization and comparison.
### Modeling: 
Applied Stratified K-Fold Cross-Validation with K-Nearest Neighbors (KNN) classifier.
### Metrics Evaluated:
Precision, recall, average precision, accuracy, and F1-score across multiple folds.


## Regression Model Comparison Project
### Objective: 
Evaluated performance of different regression models on a dataset to predict a continuous target variable.
### Baseline Model: 
Linear Regression for simplicity and baseline comparison.
### Alternative Models: 
Implemented Support Vector Regression (SVR) and Random Forest Regression for improved prediction accuracy.
### Techniques Used: 
Cross-validation, RMSE calculation, and visualization (box plots, scatter plots) to compare model performance.
### Outcome: 
Found SVR and Random Forest Regression to outperform Linear Regression in terms of RMSE and stability.


## Classification Model Evaluation Project
### Objective: 
Compared the effectiveness of Logistic Regression, Support Vector Machine (SVM), and Random Forest Classifier for binary classification.
### Techniques Used: 
GridSearchCV for hyperparameter tuning, cross-validation, and performance metrics such as accuracy, precision-recall curves, and ROC curves.
### Modeling: 
Utilized StratifiedKFold to ensure balanced class distribution across folds.
### Outcome: 
Demonstrated the robustness and predictive accuracy of Random Forest Classifier, while highlighting the efficiency of Logistic Regression and SVM for various dataset complexities.
